<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Expires" content="0">
    <title>Natural Language Processing - Notes</title>
    <link rel="stylesheet" href="../assets/style.css?v=1759443501" />
    <style>
      /* Additional styles for notes page - always visible */
      .notes-content {
        display: block !important;
      }
      
      /* aman.ai inspired styles */
      .notes-header {
        text-align: center;
        margin: 2rem 0;
      }
      
      .notes-header h1 {
        font-size: 2.5rem;
        margin-bottom: 0.5rem;
      }
      
      .notes-header .subtitle {
        color: #666;
        font-style: italic;
      }
      
      .category-section {
        background: white;
        padding: 1.5rem;
        margin-bottom: 2rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
      }
      
      .category-section h2 {
        font-size: 1.5rem;
        margin-bottom: 1rem;
        padding-bottom: 0.5rem;
        border-bottom: 2px solid #007bff;
      }
      
      .back-link {
        display: inline-block;
        margin-bottom: 1rem;
        color: #007bff;
        text-decoration: none;
        font-weight: 500;
      }
      
      .back-link:hover {
        text-decoration: underline;
      }
      
      /* Table of Contents Styles */
      .table-of-contents {
        background: #f8f9fa;
        border: 1px solid #e9ecef;
        border-radius: 8px;
        padding: 1.5rem;
        margin-bottom: 2rem;
      }
      
      .table-of-contents h3 {
        margin-top: 0;
        margin-bottom: 1rem;
        color: #495057;
        font-size: 1.2rem;
      }
      
      .toc-list {
        list-style: none;
        padding-left: 0;
        margin: 0;
      }
      
      .toc-list li {
        margin-bottom: 0.5rem;
      }
      
      .toc-list a {
        color: #007bff;
        text-decoration: none;
        font-weight: 500;
        display: block;
        padding: 0.25rem 0;
        border-radius: 4px;
        transition: all 0.2s ease;
      }
      
      .toc-list a:hover {
        background-color: #e7f3ff;
        padding-left: 0.5rem;
        text-decoration: none;
      }
      
      .question {
        margin-top: 2rem;
        margin-bottom: 1rem;
        padding-bottom: 0.5rem;
        border-bottom: 1px solid #e9ecef;
      }
      
      /* Back to Top Button */
      .back-to-top {
        position: fixed;
        bottom: 30px;
        right: 30px;
        background: #007bff;
        color: white;
        border: none;
        border-radius: 50%;
        width: 50px;
        height: 50px;
        font-size: 18px;
        cursor: pointer;
        box-shadow: 0 4px 12px rgba(0,123,255,0.3);
        transition: all 0.3s ease;
        opacity: 0;
        visibility: hidden;
        z-index: 1000;
      }
      
      .back-to-top:hover {
        background: #0056b3;
        transform: translateY(-2px);
        box-shadow: 0 6px 16px rgba(0,123,255,0.4);
      }
      
      .back-to-top.visible {
        opacity: 1;
        visibility: visible;
      }
      
      .back-to-top .icon {
        display: inline-block;
        transform: rotate(-90deg);
      }
      
      /* References section styling */
      .references-section {
        background: #f8f9fa;
        border: 1px solid #e9ecef;
        border-radius: 8px;
        padding: 1.5rem;
        margin-top: 2rem;
      }
      
      .references-section h3 {
        margin-top: 0;
        margin-bottom: 1rem;
        color: #495057;
        font-size: 1.2rem;
      }
      
      .references-list {
        margin: 0;
        padding-left: 1.5rem;
      }
      
      .references-list li {
        margin-bottom: 0.5rem;
        line-height: 1.5;
      }
      
      .references-list a {
        color: #007bff;
        text-decoration: none;
        font-weight: 500;
      }
      
      .references-list a:hover {
        text-decoration: underline;
      }
      
      @media (max-width: 768px) {
        .notes-header h1 {
          font-size: 2rem;
        }
        .table-of-contents {
          padding: 1rem;
        }
        .back-to-top {
          bottom: 20px;
          right: 20px;
          width: 45px;
          height: 45px;
          font-size: 16px;
        }
      }
    </style>
  </head>
  <body>
    <!-- Notes Content -->
    <div class="notes-content" id="notesContent">
      <header>
        <nav>
          <ul>
            <li><a href="../pages/index.html">Home</a></li>
            <li><a href="../pages/projects.html">Projects and Research</a></li>
            <li><a href="../pages/highlights.html">Highlights</a></li>
            <li><a href="../pages/notes.html">Notes Hub</a></li>
          </ul>
        </nav>
      </header>
      
      <main>
        <div class="notes-header">
          <h1>Natural Language Processing</h1>
          <p class="subtitle">comprehensive revision notes</p>
        </div>

        <div class="category-section">
          <a href="../pages/notes.html" class="back-link">‚Üê Back to Notes Hub</a>
          <h2>Natural Language Processing</h2>
          <div style="line-height: 1.8;">
            <div class="table-of-contents">
              <h3>üìã Table of Contents</h3>
              <ul class="toc-list">
                <li><a href="#q-general">1. General</a></li>
                <li><a href="#q-models--architectures">2. Models & Architectures:</a></li>
                <li><a href="#q-tasks-in-nlp">3. Tasks in NLP</a></li>
                <li><a href="#q-metrics-in-nlp-tasks">4. Metrics in NLP tasks</a></li>
                <li><a href="#q-embedding-generation-in-nlp">5. Embedding Generation in NLP</a></li>
              </ul>
            </div>

            <p id="q-general" class="question"><strong>General</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>Traditional NLP built on Bag of Words approach was limited to tasks like parsing text, sentiment analysis, topic models etc.</li>
              <li>With the emergence of Word vectors and Neural Language models, new applications like Machine Translation, entity recognition, information retrieval came into prominence.</li>
              <li>In last couple of years, the emergence of Pre-trained models like BERT, Roberta, Distilbert, Albert etc. and supporting frameworks like Hugging Face, Spacy Transformers have made NLP tasks like Reading Comprehension, Text Summarization etc. possible and state of the art benchmarks were created by these NLP models.</li>
            </ul>
            <p id="q-models--architectures" class="question"><strong>Models & Architectures:</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>GPT-3 Model</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Built based on autoregressive model technique</li>
                    <ul style="margin-left: 1.5rem;">
                      <li>Guess the next token after reading all the previous ones</li>
                    </ul>
                  <li>Larger architecture relative to BERT [13]</li>
                  <li>Model Size: The largest GPT-3 model has 175 billion parameter. This is 470 times bigger than the largest BERT model (375 million parameters)</li>
                  <li>GPT-3 is an Autoregressive model and follows a decoder only architecture. It is trained using next word prediction objective</li>
                  <li>Learning: GPT-3 learns through Few Shots and there is no Gradient updates while learning</li>
                  <li>Training Data Needed: GPT-3 needs less training data. It can learn from very less data and this enables its application on domains having less data</li>
                </ul>
              <li>BERT model</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Built based on Autoencoding model technique  [13]</li>
                    <ul style="margin-left: 1.5rem;">
                      <li>Builds a bidirectional representation of the whole sentence</li>
                      <li>Most natural application is sentence classification or token classification</li>
                    </ul>
                  <li>BERT model is heavily dependent on fine tuning</li>
                  <li>Dependency of fine-tuning on large datasets</li>
                </ul>
              <li>Transformers</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Transformers is a family of sequence (takes an input sequence) to sequence models (outputs a sequence) that is very useful in NLP application (especially translation) because of the parallelism it provides and its ability to utilize distributed systems effectively.</li>
                  <li>The basic architecture consist of:</li>
                    <ul style="margin-left: 1.5rem;">
                      <li>encoding (input), a multi-head attention network, layer norm and residual connection (add and norm)</li>
                      <li>decoding network -> masked form of the encoding architecture</li>
                    </ul>
                  <li>On the encoding side the input embedding + a positional encoding (add positional information to it) is then passed into an attention network (to add some context).</li>
                  <li>On the decoding side we have the output embedding +  a positional encoding which is then passed into a masked multi-head attention network</li>
                  <li>At this point, after the both the encoding and decoding side have gone through the attention and masked attention network respectively, they are passed into a encoding-decoding multi head cross attention then through a feed forward network before going through a linear and Softmax activation function which outputs probability for the best output translation</li>
                </ul>
            </ul>
            <p id="q-tasks-in-nlp" class="question"><strong>Tasks in NLP</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>Summarization: this can be either abstractive or extractive summarization.  Extractive summarization involves concatenation of pieces of text to form a summary. Abstractive summary is based phrases to capture the real essence of the original text. This is typically done with sequence to sequence models using transformers BERT or GPT.</li>
              <li>Classification Type: Named Entity Recognition, Sentiment Analysis, Text classification</li>
              <li>Machine Translation: Uses mostly sequence to sequence models example of models are: Transformers, Facebook's M2M-100 and Google's Multilingual BERT.</li>
              <li>Query Understanding and Retrieval: This involves parsing and understanding user queries, especially in search engines or conversational agents, to determine the user's intent and relevant context.</li>
            </ul>
            <p id="q-metrics-in-nlp-tasks" class="question"><strong>Metrics in NLP tasks</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>Rouge (Recall Oriented Understudy of Gisting Evaluations) : commonly used in text summarization. Rouge-N measures the overlap of n-grams between the generated and the reference texts. Common variants are Rouge-N(n grams), Rouge-S (skip gram), Rouge-L (longest common subsequence)</li>
              <li>Bleu: Used mostly for machine translation and measures overlap between machine translation and reference.</li>
              <li>Bert score: Another similarity score that uses the Bert family of models to measure similarity between two texts or sentences.</li>
              <li>Factual consistency metrics: this are different from similar metrics since they are designed to measure factual consistent and combat hallucinations. Example FEVER score which measures the support a text has from a given evidence. Also there's the FACT CC model that can help us understand factual consistency of text generated from summarization models.</li>
            </ul>
            <p id="q-embedding-generation-in-nlp" class="question"><strong>Embedding Generation in NLP</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>Text preprocessing(First step)</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Normalization: Lowercasing, Punctuation removal, lemmatization, Strip Accents, Normalization Form KD (NFKD)</li>
                  <li>Tokenization: ngram, subword, character tokenization</li>
                  <li>Tokens to id</li>
                </ul>
              <li>Embedding generation (Second Step): To generate embddings there are different methods.</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Statistical methods: These are based on bag of words (matrix of sentence by dictionary) and TFIDF which attempt to normalize word frequency with the frequency off the word in the document /corpus. However more recently there are advanced ways tog enrage embeddings that leverage. The shortcoming of these is that it does not consider the order of words in a sentence. Thereby losing the semantic meaning.</li>
                  <li>Shadow Neural Network aka Word-to-Vec e.g Continuous Bag of Words (CBOW), Skipgram</li>
                  <li>Embedding from Pretrained Networks using an embedding or look up layer</li>
                  <li>Directly from Transformer Architectures e.g Google's BERT, Open AI's GPT, BLOOM. DistilBert is the efficient version of Bert that is smaller and faster and performs on a wide range of tasks, For multilingual capability we can use DistilmBERT.</li>
                </ul>
            </ul>
          </div>
        </div>

      </main>
      
      <footer>
        <p>&copy; 2025 Natural Language Processing Notes</p>
      </footer>
    </div>

    <!-- Back to Top Button -->
    <button class="back-to-top" id="backToTop" onclick="scrollToTop()" title="Back to Table of Contents">
      <span class="icon">‚Üí</span>
    </button>

    <script>
      // Notes are now freely accessible - no authentication required
      function checkAuth() {
        // Always show content - knowledge is free!
        document.getElementById('notesContent').style.display = 'block';
      }

      // Scroll to top functionality
      function scrollToTop() {
        window.scrollTo({
          top: 0,
          behavior: 'smooth'
        });
      }

      // Show/hide back to top button based on scroll position
      function toggleBackToTopButton() {
        const backToTopButton = document.getElementById('backToTop');
        if (window.pageYOffset > 300) {
          backToTopButton.classList.add('visible');
        } else {
          backToTopButton.classList.remove('visible');
        }
      }

      // Initialize page
      document.addEventListener('DOMContentLoaded', function() {
        checkAuth();
        
        // Add scroll event listener for back to top button
        window.addEventListener('scroll', toggleBackToTopButton);
      });
    </script>
  </body>
</html>