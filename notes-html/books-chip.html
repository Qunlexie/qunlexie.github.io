<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Expires" content="0">
    <title>[Book] Designing ML Systems By Chip Huyen - Notes</title>
    <link rel="stylesheet" href="../assets/style.css?v=1759443501" />
    <style>
      /* Additional styles for notes page - always visible */
      .notes-content {
        display: block !important;
      }
      
      /* aman.ai inspired styles */
      .notes-header {
        text-align: center;
        margin: 2rem 0;
      }
      
      .notes-header h1 {
        font-size: 2.5rem;
        margin-bottom: 0.5rem;
      }
      
      .notes-header .subtitle {
        color: #666;
        font-style: italic;
      }
      
      .category-section {
        background: white;
        padding: 1.5rem;
        margin-bottom: 2rem;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
      }
      
      .category-section h2 {
        font-size: 1.5rem;
        margin-bottom: 1rem;
        padding-bottom: 0.5rem;
        border-bottom: 2px solid #007bff;
      }
      
      .back-link {
        display: inline-block;
        margin-bottom: 1rem;
        color: #007bff;
        text-decoration: none;
        font-weight: 500;
      }
      
      .back-link:hover {
        text-decoration: underline;
      }
      
      /* Table of Contents Styles */
      .table-of-contents {
        background: #f8f9fa;
        border: 1px solid #e9ecef;
        border-radius: 8px;
        padding: 1.5rem;
        margin-bottom: 2rem;
      }
      
      .table-of-contents h3 {
        margin-top: 0;
        margin-bottom: 1rem;
        color: #495057;
        font-size: 1.2rem;
      }
      
      .toc-list {
        list-style: none;
        padding-left: 0;
        margin: 0;
      }
      
      .toc-list li {
        margin-bottom: 0.5rem;
      }
      
      .toc-list a {
        color: #007bff;
        text-decoration: none;
        font-weight: 500;
        display: block;
        padding: 0.25rem 0;
        border-radius: 4px;
        transition: all 0.2s ease;
      }
      
      .toc-list a:hover {
        background-color: #e7f3ff;
        padding-left: 0.5rem;
        text-decoration: none;
      }
      
      .question {
        margin-top: 2rem;
        margin-bottom: 1rem;
        padding-bottom: 0.5rem;
        border-bottom: 1px solid #e9ecef;
      }
      
      /* Back to Top Button */
      .back-to-top {
        position: fixed;
        bottom: 30px;
        right: 30px;
        background: #007bff;
        color: white;
        border: none;
        border-radius: 50%;
        width: 50px;
        height: 50px;
        font-size: 18px;
        cursor: pointer;
        box-shadow: 0 4px 12px rgba(0,123,255,0.3);
        transition: all 0.3s ease;
        opacity: 0;
        visibility: hidden;
        z-index: 1000;
      }
      
      .back-to-top:hover {
        background: #0056b3;
        transform: translateY(-2px);
        box-shadow: 0 6px 16px rgba(0,123,255,0.4);
      }
      
      .back-to-top.visible {
        opacity: 1;
        visibility: visible;
      }
      
      .back-to-top .icon {
        display: inline-block;
        transform: rotate(-90deg);
      }
      
      /* References section styling */
      .references-section {
        background: #f8f9fa;
        border: 1px solid #e9ecef;
        border-radius: 8px;
        padding: 1.5rem;
        margin-top: 2rem;
      }
      
      .references-section h3 {
        margin-top: 0;
        margin-bottom: 1rem;
        color: #495057;
        font-size: 1.2rem;
      }
      
      .references-list {
        margin: 0;
        padding-left: 1.5rem;
      }
      
      .references-list li {
        margin-bottom: 0.5rem;
        line-height: 1.5;
      }
      
      .references-list a {
        color: #007bff;
        text-decoration: none;
        font-weight: 500;
      }
      
      .references-list a:hover {
        text-decoration: underline;
      }
      
      @media (max-width: 768px) {
        .notes-header h1 {
          font-size: 2rem;
        }
        .table-of-contents {
          padding: 1rem;
        }
        .back-to-top {
          bottom: 20px;
          right: 20px;
          width: 45px;
          height: 45px;
          font-size: 16px;
        }
      }
    </style>
  </head>
  <body>
    <!-- Notes Content -->
    <div class="notes-content" id="notesContent">
      <header>
        <nav>
          <ul>
            <li><a href="../pages/index.html">Home</a></li>
            <li><a href="../pages/projects.html">Projects and Research</a></li>
            <li><a href="../pages/highlights.html">Highlights</a></li>
            <li><a href="../pages/notes.html">Notes Hub</a></li>
          </ul>
        </nav>
      </header>
      
      <main>
        <div class="notes-header">
          <h1>[Book] Designing ML Systems By Chip Huyen</h1>
          <p class="subtitle">comprehensive revision notes</p>
        </div>

        <div class="category-section">
          <a href="../pages/notes.html" class="back-link">‚Üê Back to Notes Hub</a>
          <h2>[Book] Designing ML Systems By Chip Huyen</h2>
          <div style="line-height: 1.8;">
            <div class="table-of-contents">
              <h3>üìã Table of Contents</h3>
              <ul class="toc-list">
                <li><a href="#q-chapter-11--the-human-side-of-">1. Chapter 11 - The human side of Machine Learning</a></li>
                <li><a href="#q-chapter-10--infrastructure-and">2. Chapter 10 - Infrastructure and tooling for ML Ops</a></li>
                <li><a href="#q-chapter-9---continual-learning">3. Chapter 9 -  Continual Learning & Test in Production</a></li>
                <li><a href="#q-chapter-7--model-deployment-an">4. Chapter 7 - Model deployment and prediction service</a></li>
                <li><a href="#q-chapter-6--model-development-a">5. Chapter 6 - Model development and offline evaluation</a></li>
                <li><a href="#q-chapter-5--feature-engineering">6. Chapter 5 - Feature Engineering (FE)</a></li>
                <li><a href="#q-chapter-4--training-data">7. Chapter 4 - Training Data</a></li>
                <li><a href="#q-chapter-3--data-engineering-fu">8. Chapter 3 - Data Engineering Fundamentals</a></li>
                <li><a href="#q-chapter-2--intro-to-ml-system-">9. Chapter 2 - Intro to ML system design</a></li>
                <li><a href="#q-chapter-1--overview-of-ml-syst">10. Chapter 1 - Overview of ML systems</a></li>
              </ul>
            </div>

            <p id="q-chapter-11--the-human-side-of-" class="question"><strong>Chapter 11 - The human side of Machine Learning</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>User Experience:</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Smooth failing: trade-off between accuracy and latency. Sometimes you prefer speed over accuracy. Both can be achieved with a backup model for example when the main model takes too long to respond e.g. large language models</li>
                  <li>Sometimes predictions are mostly correct but only sometimes correct which can also affect user experience another way to overcome this is to show multiple answers and allow users to choose the best.</li>
                  <li>Sometimes ML prediction can change at each run so there's also consistency - accuracy tradeoff to minimize user experience disruptions</li>
                </ul>
              <li>Organizational Structure:</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Subject matter experts (SMEs) have a huge role in the ML process and cross-functional collaboration is key across data scientists and SMEs.</li>
                  <li>End-to-end data scientists:</li>
                    <ul style="margin-left: 1.5rem;">
                      <li>Data scientists manage the entire process</li>
                      <li>This may be good depending on the development stage of the infrastructure. The more developed infra is, the better it is to manage the end-to-end process without burdening the DS with low-level infra.</li>
                      <li>Separate team MLOps/data platform for production (DS work ends at the dev environment/mode building)</li>
                      <li>This can lead to finger-pointing, communication overhead, and debugging challenges</li>
                    </ul>
                </ul>
              <li>Responsible AI</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Ofqual(an automatic system for grading A-level exams in the UK) and Strava (running an app that mistakenly exposed military bases) are two examples of irresponsible AI. These examples RE plagues By the following problems: lack of correct model objective, lack of transparency, and lack of fine-grained mode examination.</li>
                  <li>Framework for responsible AI:</li>
                    <ul style="margin-left: 1.5rem;">
                      <li>Identify sources of model bias: this can come in the training, feature engineering, objective, and evaluation</li>
                      <li>Understand the limitations of the data approach: make sure to bring in SMEs as you build the ML model.</li>
                      <li>Understand tradeoffs: privacy- accuracy (higher privacy less accuracy especially on certain she sets) tradeoffs and compactness-fairness (higher compactness for example via pruning can cause biases on certain parts of the data) tradeoff</li>
                      <li>Act early: put privacy systems in place. Cheaper than acting late</li>
                      <li>Create model cards: this records information on the model properties and helps transparency</li>
                      <li>Establish a process for mitigating bias: internal tools for bias mitigation makes the process easier.</li>
                      <li>Staying up to date on responsible AI practices</li>
                    </ul>
                </ul>
            </ul>
            <p id="q-chapter-10--infrastructure-and" class="question"><strong>Chapter 10 - Infrastructure and tooling for ML Ops</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>Four main infrastructure components for MLOps</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Storage and compute:</li>
                    <ul style="margin-left: 1.5rem;">
                      <li>Storage: where the data is stored. Think AWS S3 or Snowflake</li>
                      <li>Compute:  compute determines the number of resources that can be used to run the workload. It is divided into two:</li>
                      <li>Memory & IO bandwidth: memory deals with how much memory is available to load the data while IO bandwidth is how fast it can load data in and out of memory</li>
                      <li>Speed of operation or CPU cores: this is how fast it can process the loaded data usually measured in FLOPS (floating point operations per second) or CPU cores. Since there is a theoretical maximum speed relative to the amount used. The utilization is used to understand how effective the computing unit is</li>
                    </ul>
                  <li>Developer environment</li>
                    <ul style="margin-left: 1.5rem;">
                      <li>Dev environment: Most companies today use cloud-standardized developer environments e.g AWS cloud9, Github Codespaces, AWS Sagemaker studio (this comes bit an in-built Jupiter notebook), or VS code where you can ssh into the cloud.</li>
                      <li>Prod Environment: Docker containers are used to reproduce the ML dev environment and states in production including the library and dependencies. Kubernetes can be used to manage multiple Docker containers especially when they are running on different hosts.</li>
                    </ul>
                  <li>Resource Management:</li>
                    <ul style="margin-left: 1.5rem;">
                      <li>Resource management involves crons, schedulers, and orchestrators</li>
                      <li>Crons & schedulers (when): crons schedule jobs without an understanding of dependency while schedulers can take care of dependencies (workflow and its dependencies  are usually represented with Dynamic acrylic graph, DAG)</li>
                      <li>Orchestrators (where): answers the question of where to run the job (instances, hosts, etc) and needs to understand if resources are available.</li>
                      <li>Data workflow management orchestrators include airflow, prefect, Kubeflow, Argo, and Metaflow.</li>
                    </ul>
                  <li>ML platform: Using ML to serve multiple purposes is the basis of an ML platform.</li>
                    <ul style="margin-left: 1.5rem;">
                      <li>Model Deployment: the most developed aspect of the ML platform used to package models and push end points to users.</li>
                      <li>Model store: store model, its parameters, dependencies, and other artifacts.</li>
                      <li>Feature store: feature management, computation, and feature consistency.</li>
                    </ul>
                </ul>
              <li>Public Cloud or Private data center: most companies now use the public cloud (e.g AWS Elastic Cloud Compute EC2) but there are important trade-offs including security, independence, cost of operating a private data center, ease of use, etc.</li>
            </ul>
            <p id="q-chapter-9---continual-learning" class="question"><strong>Chapter 9 -  Continual Learning & Test in Production</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>Continual Learning (CL) involves setting up infrastructure to update the model as required. CL is important given that a model may degrade as new data comes in.</li>
                <ul style="margin-left: 1.5rem;">
                  <li>CL can help adapt quickly to rare events and users' changing preferences, combat data distributions shifts and resolve the continuous cold start problem (where the model is unable to adapt to changing situations of the user e.g change from mobile to laptop and sees these as a new sample)</li>
                  <li>CL can stateful or stateless. Stateless is when training occurs from scratch while stateful is a retaining/refinement and update process based on past models but with new data.</li>
                  <li>Challenges of CL</li>
                    <ul style="margin-left: 1.5rem;">
                      <li>Fresh data access: speed of updates depend on how fast we can query new incoming data and obtain labels for these</li>
                      <li>Evaluation: speed of updates depends on how quickly evaluation can be done on the new model. Sometimes evaluating takes time</li>
                      <li>Algorithm: some algorithms like matrix-based and tree-based models may be harder to train very frequently since they may require stateless retraining from scratch, unlike neural network-based models which can be updated with new data easily.</li>
                      <li>CL depends on the maturity of the company and the workflow. It is usually in 4 stages starting from:</li>
                      <li>manual stateless retraining (no automation and training from scratch)</li>
                      <li>automated stateless retraining workflow (automated but trained from scratch e.g. based on a frequency schedule)</li>
                      <li>automated stateful retaining (here you don't train from scratch anymore  based on frequency, performance, drift, or new data volume)</li>
                      <li>continuous learning (here in stage 4 you need a mechanism for monitoring model performance, triggering, evaluating, and applying model updates)</li>
                    </ul>
                  <li>CL updates to the old model should be based on the value of the fresh data which can be understood via rigorous evaluation (e.g. times series train-test evaluation type methods aka backtesting). Also, the choice of data update or model update or both can be made using experiments.</li>
                </ul>
              <li>Test in Production:</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Shadow Deployment: make predictions using two different models but only serve the existing model prediction to the user. Cons: double cost</li>
                  <li>A/B testing: serve a percentage of real traffic at random to the candidate model and test the statistical significance of the difference between the candidate model and the existing model. Cons: need a lot of samples for strong statistical significance</li>
                  <li>Canary Releases: similar to A/B testing but slowly serve portions of the traffic to the user and increase the percentage of traffic gradually if the performance is satisfactory otherwise abandon and serve the existing model only</li>
                  <li>Interleaving experiments: serve both models simultaneously and let users figure out the best model. Pros: reduced samples compared to A/B testing</li>
                  <li>Bandit algorithm: similar to reinforcement learning, make the model balance between exploration (finding a new better model) and exploitation (using the current best model) for several candidate models. This requires a short feedback loop and checkpoint of the performance of the models.</li>
                </ul>
              <li>In general testing in production is a very good way of ensuring model integrity but should be carefully done and preferably automated and clearly stated before the pipeline is built.</li>
              <li>Chapter 8- Data distribution shifts and monitoring</li>
              <li>ML models can fail for two main reasons:</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Software system failure (deployment, hardware, server, dependency). System failures are usually easier to detect since they are accompanied by operational breakage or error codes</li>
                  <li>ML-specific failure (edge cases, production data differing from training data, degenerate feedback loop). ML modes on the other hand can fail silently</li>
                </ul>
              <li>ML-specific failures:</li>
                <ul style="margin-left: 1.5rem;">
                  <li>ML models in production can fail since they assume that real-world data will the similar to training data and in most cases is not. Another assumption is the stationary assumption where production data is non-stationary unlike the training data</li>
                  <li>Edge cases: Edge cases can also affect data e.g. autonomous vehicles that are unsafe 0.01%. Outlier is data related (e.g a person jay-walking) while the edge case is performance related (if the autonomous vehicle adjusts well to the  jay-walking outlier then it's not an edge case otherwise it is)</li>
                  <li>A degenerate feedback loop (DFL) can cause an ML system to fail and this occurs when biased feedback is used as input to an ML model. For example, a recommendation system that recommends the most popular items simply because they are ranked top and these are being shown to users as more friendly so the model has wrongly learned that those are the popular items.</li>
                    <ul style="margin-left: 1.5rem;">
                      <li>Detecting DFL: The homogeneity of online prediction can point to degenerate feedback loops. We can measure the degenerate feedback loop offline based on the popularity of the output of the system</li>
                      <li>Correcting DFL:</li>
                      <li>DFL can be corrected by randomization (for example serve initially random recommendations and let the system figure out the best though this may affect user experience)</li>
                      <li>Another approach is to use positional features to weigh the importance of recommission's position to the output variable</li>
                    </ul>
                  <li>Data distribution shift:</li>
                    <ul style="margin-left: 1.5rem;">
                      <li>three types including covariate shift, label shift, and concept drift.</li>
                      <li>Covariate shift: output probability P(Y|X) remains the same but input distribution P(X) changes. This may occur because of data bias in inference vs training</li>
                      <li>Label shift: P(Y) changes but P(X|Y) remains the same. Label shift and covariants shift usually occur simultaneously but not necessarily.</li>
                      <li>Concept Drift: P(Y|X) changes but P(X) remains the same. This in practice is usually due to seasonal variations in observed output.</li>
                      <li>other types include outright feature change or label schemes change (when characteristics of the label change such as from 0-10 to 1-9 for a ranking label)</li>
                      <li>Detecting data distribution shift: several techniques can be used to detect data shift including statistical methods such as measuring the P10, P50, etc as well as two sample tests to compare distribution (less used in industry as of now). When doing this it is also important to understand the temporal characteristics (daily, weekly) of the data as this may affect the statistics we are seeing.</li>
                      <li>Addressing data distributions shift: train big models (that potentially capture unknown distributions), causal inference or unsupervised adaptation without requiring new labels (research area that is growing), retrain model (either from scratch or via fine-tuning aka domain adaptation/transfer learning)</li>
                    </ul>
                </ul>
              <li>Monitoring and observability:</li>
                <ul style="margin-left: 1.5rem;">
                  <li>monitoring should encompass both software system information (e.g uptime - how often the system is online, latency, throughput, CPU utilization) and ML system (accuracy, feature, raw input, prediction).</li>
                  <li>The monitoring toolkit should include logs, dashboards, and alerts that can capture the monitoring elements of log traces and metrics.</li>
                  <li>Observability involves setting up your system to be able to understand the internal workings of the system. This usually goes hand in hand with monitoring.</li>
                </ul>
            </ul>
            <p id="q-chapter-7--model-deployment-an" class="question"><strong>Chapter 7 - Model deployment and prediction service</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>Model deployment in "production" can vary depending on what production means to different companies. For some, it might mean a notebook to play around with which is very similar to a "development" environment. For others, it's a service that services millions of users daily. In the second case, the hard part is not building the model but: latency, 99% uptime (reliability), monitoring, debugging, and updating.</li>
              <li>ML deployment: four key takeaways which can be summarized as</li>
                <ul style="margin-left: 1.5rem;">
                  <li>ML deployment involves more than one or two models at a time</li>
                  <li>ML model performance decreases over time</li>
                  <li>ML retraining happens very often</li>
                  <li>ML engineers need to consider the scale</li>
                </ul>
              <li>Batch Vs online prediction service:</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Batch prediction involves using only batch features (from a database or warehouse) to serve prediction. This is good for high volume high throughput prediction that does not have to be served immediately. e.g. Netflix recommendations can be pre-computed for all users before time and served only when users log in. A drawback of Batch prediction is that they may be less responsive to user preferences</li>
                  <li>Online prediction can be divided into two: online prediction using Batch features (precomputed embedding) or online prediction using both batch and streaming features (known as streaming prediction). In general online predictions are generated as soon as requests are sent. Think about google translate.</li>
                </ul>
              <li>Model compression: four types including:</li>
                <ul style="margin-left: 1.5rem;">
                  <li>low-rank factorization where the high dimensional tensors are replaced with low dimensional ones. Good example being mobile nets where the convolution filters (K-K-C) are reduced to depth-wise (K-K) and point-wise filters giving K-K +C</li>
                  <li>Knowledge distillation: using a pre-trained teacher model (denser) to train a lighter model (student). An example is DistilBERT which is 60% faster than BERT.</li>
                  <li>Pruning: In tree models, it means removing nodes in a tree. For neural networks, it may mean either traducing architecture parameters or setting least useful to parameters zero.</li>
                  <li>Quantization: reducing the precision of parameters of the model. This is the most commonly used method of compression. A good example is converting 32-bit float to 16Bit float or 8-bit integer.</li>
                </ul>
              <li>ML on the cloud, edge devices and browsers :</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Most ML today run on the cloud however there are advantages to running locally on the device (edge) hardware including network latency, and cloud cost.</li>
                  <li>To run on edge devices in the past, framework developers (such as PyTorch) world normally build for a particular type of hardware type (CPU, GPU or TPU) and backend. However, nowadays intermediate representation (IR) is used as a "middleman" to convert generic models to machine code by "lowering" the high model to IRs.</li>
                  <li>In many cases after lowering the code to run on model you still need to optimize the code to run on the hardware efficiently. This can either be done locally (optimize a set of operators in the model) or globally (optimizing the brute model computational graph). One can also use ML to optimize the computational graph (examples of software available for this: cuDNN auto-tune and autoTVM)</li>
                  <li>These days running models in browsers has been big and the most promising approach uses Web Assembly (WASM) compiling built models (used sklearn, PyTorch, etc) on WASN to obtain an executable file that can be run on the web with JavaScript (since JavaScript itself is to slow for compiling ml tasks)</li>
                </ul>
            </ul>
            <p id="q-chapter-6--model-development-a" class="question"><strong>Chapter 6 - Model development and offline evaluation</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>Classical ML algorithms are not going away and will still be used in tandem with deep learning in the future. Latency and compute considerations may make one choose classical/simple ML methods over DL.</li>
              <li>6 Tips for model selection include: avoiding state of the art trap, Starting with simple models, avoiding human biases in model selection, tracking performance as training data increases, trade off of different models (e.g interpretability and latency),  and model assumptions (e.g predictability, independence, normal distribution etc.</li>
              <li>Ensembling: this method is best when underlying models are uncorrelated. 3 types of ensembles: Bagging (training a model on random subsets of the same data and aggregate) Boosting (improving a weak learner by weighing the output prediction) and stacking (combining outputs of different models)</li>
              <li>Experiment tracking (by monitoring by loss, model performance, output, and hyperparameters over time) and versioning (ability to replicate and revert to a change or interaction during an ML experiment). Versioning is hard since ML is part code part data making it hard to reverse (since data is harder to track than code)</li>
              <li>Many reasons why ML models fail include assumption violation, poor hyperparameters, data and feature problems, poor model implementation etc. some ways to avoid these include starting simple, trying to overfit a single batch(try to observe theoretical high accuracy -100% or very low loss), setting seed to ensure stability.</li>
              <li>Distributed Training:</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Data parallelism: split data across different machines. Can be synchronous (model wait for gradient from all machines before updating weight. cons: slowing down of a worker or machine can delay the system) or asynchronous (weight update is done with a gradient from each machine)</li>
                  <li>Model parallelism: different model components run on different machines. However, sometimes Machine A might have to wait for machine B if it's multilayered NN.</li>
                </ul>
              <li>AutoML: Two types:</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Soft AutoML(hyperparameter tuning): finding the best hyperparameter by building model on the training set and testing on the validation set.</li>
                  <li>Hard AutoML (architecture search and learned optimizer): A good example is neural architecture search  Which involves designing a search space (model building block), performance estimation strategy (smartly evaluate modes without waiting till convergence), and search strategy (random forest or more proffered reinforcement learning). We can also train optimizers to be better at certain tasks over the hand-built ones such as Adam and SGD. an example is efficientNets by Google.</li>
                </ul>
              <li>Model offline evaluation:</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Baselines: very useful to compare the performance of a model offline since performance metrics such as accuracy or F1 score or FID score (for GANs) mean nothing without correct comparison. Many ways to generate baselines including random, heuristic, existing model comparison, or human baselines.</li>
                  <li>Evaluation methods: many evaluation methods aside from accuracy and F1: perturbation (adding small changes to the training set to understand performance with small distortions), model calibration (making sure model output represents real expected observation), slice-based (examining performance on a different slice of data), directional expectation (checking expectation balance observed prediction based on data whose outputs are well known), invariance (model output consistent with a minor change in the input) and confidence test.</li>
                </ul>
            </ul>
            <p id="q-chapter-5--feature-engineering" class="question"><strong>Chapter 5 - Feature Engineering (FE)</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>Feature engineering is important in many ML workflows despite the progress of deep learning where correct features can be learnt.</li>
              <li>For quantitative features common FE methods include handling missing values (via detection or imputation), scaling (normalization to make it homogenous (e.g using log), standardization to convert to normal distribution), discretization</li>
              <li>For categorical features, you can do encoding (preferably hashing trick for production use cases), positional embedding (more common in NLP), feature crossing</li>
              <li>Data leakage is when label information on the test set leaks into the training data.</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Ways to prevent this included splitting before scaling, time-dependent split for time-dependent variables, splitting before missing data fill, duplication, and leak from data generation (more common with images).</li>
                  <li>Ways to detect data leakage include understanding how data is generated; understanding feature importance (e.g. using Shapely additive explanations SHAP which is model agnostic)</li>
                </ul>
              <li>Engineering good feature involve using features that generalize well (capturing feature coverage [in test and train set] and distribution within each set) and removing useless features to avoid feature dimensionality curse</li>
            </ul>
            <p id="q-chapter-4--training-data" class="question"><strong>Chapter 4 - Training Data</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>Sampling can be non-probabilistic aka biased (convenience, quota, judgment, snowball) or random (stratified, weighted, importance, reservoir - common for stream data). Training data should be backed by random sampling.</li>
              <li>ML problems can have hand labels, natural labels (using feedback from users), or no labels. Hand labeling (via human annotators) can be expensive/need domain expertise & slow while presenting label multiplicity (conflict among different annotators) and data lineage problems. Natural labels need to consider the length of the feedback loop. In the absence of labels, weak learning (weak labels using labeling functions/heuristics), active learning (mix of the weak label and hand labeling for only the difficult case), semi-supervised learning (predicting labels using a small set of data) and transfer learning can come in handy.</li>
              <li>Most ML algorithms love balanced classes though most real-world practical datasets are imbalanced. Three main ways to  ways to combat this - Data, metric, algorithm/model:</li>
                <ul style="margin-left: 1.5rem;">
                  <li>Data: oversampling minority or undersampling majority. several variations of this may include SMOTE (synthetic minority oversampling technique)</li>
                  <li>Metric: rather than accuracy use F1, recall, precision, AUC.</li>
                  <li>Model: cost-sensitive learning, class balanced loss, focal loss. All these are based on tweaking the loss function to interest, minority or difficult classes respectively.</li>
                </ul>
              <li>Data Augmentation: transformations (common with CV), perturbation (introducing noise), data synthesis (creating examples/data)</li>
            </ul>
            <p id="q-chapter-3--data-engineering-fu" class="question"><strong>Chapter 3 - Data Engineering Fundamentals</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>Data model and data formats are things to consider when handling data. A data format is either binary or text and column or row-major formats. The data model includes relational (SQL) and the NoSQL(document or graph) models.</li>
              <li>Two types of data processing: transactional - updated and inserted into tables to produce output (OLTP), and analytical - involving manipulations to produce output (OLAP). These days databases can handle both</li>
              <li>Modes of passing data vary. However, it is usually through three modes. Through databases, data warehouse (a system for structured data), or through real-time transport (Apache Kafka, Rabbit MQ). There is also a growing trend of a data lake which is used for mostly raw/unstructured data.</li>
              <li>ETL: extract, transform and load is a way of moving data from data input sources to output targets (warehouse or Database).</li>
              <li>ACID: Atomicity, consistency, isolation, and durability is a key characteristic of any transactional database.</li>
            </ul>
            <p id="q-chapter-2--intro-to-ml-system-" class="question"><strong>Chapter 2 - Intro to ML system design</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>ML metrics need to correlate with business metrics for it to be useful to businesses.</li>
              <li>Scalability, maintainability, reliability, and adaptability are the 4 key requirements for an ML system</li>
              <li>ML process is iterative but usually involves the following: problem framing, data engineering, model building, deployment, continual learning, business analysis</li>
              <li>Classification and regression are the two broad types of ML problems. Multi-label classification is one of the most tricky types of classification problems (multiple labels e.g predicting the type of an article in which case it can both be tech and entertainment).</li>
            </ul>
            <p id="q-chapter-1--overview-of-ml-syst" class="question"><strong>Chapter 1 - Overview of ML systems</strong></p>
            <ul style="margin-left: 1.5rem;">
              <li>ML can solve many problems but not all problems. Data, patterns, and similarity between training and unseen data are some of the things that need to be satisfied before ML can be deployed.</li>
              <li>ML in production needs to consider computational complexity, multiple stakeholder priorities, fairness, interpretability</li>
              <li>ML in production cannot just be about the model. Data stack, Deployment, monitoring, and the entire infra need to be considered.</li>
            </ul>
          </div>
        </div>

      </main>
      
      <footer>
        <p>&copy; 2025 [Book] Designing ML Systems By Chip Huyen Notes</p>
      </footer>
    </div>

    <!-- Back to Top Button -->
    <button class="back-to-top" id="backToTop" onclick="scrollToTop()" title="Back to Table of Contents">
      <span class="icon">‚Üí</span>
    </button>

    <script>
      // Notes are now freely accessible - no authentication required
      function checkAuth() {
        // Always show content - knowledge is free!
        document.getElementById('notesContent').style.display = 'block';
      }

      // Scroll to top functionality
      function scrollToTop() {
        window.scrollTo({
          top: 0,
          behavior: 'smooth'
        });
      }

      // Show/hide back to top button based on scroll position
      function toggleBackToTopButton() {
        const backToTopButton = document.getElementById('backToTop');
        if (window.pageYOffset > 300) {
          backToTopButton.classList.add('visible');
        } else {
          backToTopButton.classList.remove('visible');
        }
      }

      // Initialize page
      document.addEventListener('DOMContentLoaded', function() {
        checkAuth();
        
        // Add scroll event listener for back to top button
        window.addEventListener('scroll', toggleBackToTopButton);
      });
    </script>
  </body>
</html>